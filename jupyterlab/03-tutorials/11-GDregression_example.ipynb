{
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": "py",
      "mimetype": "text/x-python",
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "cell_type": "code",
      "id": "css_setup",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        }
      },
      "source": [
        "import requests\n",
        "from IPython.core.display import HTML\n",
        "HTML(f\"\"\"\n",
        "<style>\n",
        "@import \"https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css\";\n",
        "</style>\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "iGSodREog9DPslK6vaFkb",
      "metadata": {},
      "source": [
        "# Gradient Descent / Ascent\n",
        "This notebook is for illustrating the plain vanilla gradient descent algorithm. In practice you would likely use libraries with more sophisticated implementations such as pytorch or scikitlearn.\n",
        "## Gradient Ascent on a function\n",
        "This example shows a basic implementation of the gradient ascent algorithm.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "C92lM5tqKhU5a3x9VlU5P",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "plt.style.use('bmh')\n",
        "from IPython.core.display import HTML as Center\n",
        "import itertools as itr\n",
        "\n",
        "Center(\"\"\" <style>\n",
        ".output_png {\n",
        "    display: table-cell;\n",
        "    text-align: center;\n",
        "    vertical-align: middle;\n",
        "}\n",
        "</style> \"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "IESkM3tgyerH1KchC7HFX",
      "metadata": {},
      "source": [
        "Define a bivariate (two-variables) function, $f(x,y)=\\exp(-(x-2)^2 - (y+1)^2)$, to **maximize** . The true **maximum** is easily seen to be at $x=2$ and $y=-1$.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "zp6WLbqTTG56KxhBkOdB5",
      "metadata": {},
      "source": [
        "def f(x, y):\n",
        "    return np.exp(-(x-2)**2 - (y+1)**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "N1x19ZUYFWQj_ribppuLv",
      "metadata": {},
      "source": [
        "Let's make a surface plot to  get a feel for the function.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "y4Q1zn-ygX-Lm7-7F_7kV",
      "metadata": {},
      "source": [
        "# The ranges of x and y values that we'd like to include in the contour plot.\n",
        "x_range = np.linspace(-1, 5, 40)\n",
        "y_range = np.linspace(-3, 3, 40)\n",
        "\n",
        "# Form all possible x-y pairs in these ranges.\n",
        "X, Y = np.meshgrid(x_range, y_range)\n",
        "\n",
        "# Make the surface plot.\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X, Y, f(X, Y), cmap='coolwarm')\n",
        "\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "#ax.set_zlabel('f(x, y)')\n",
        "ax.set_title(r'$f(x,y)=\\exp(-(x-2)^2 - (y+1)^2)$');\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "kSBfmVX3puLZdwOY-Lt6E",
      "metadata": {},
      "source": [
        "Let's also make a _contour plot_ that shows the surface of the function $f$. (Dark rings = lower values, light rings = higher values.)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "D3H0o-suZbkQDf8pJLUsA",
      "metadata": {},
      "source": [
        "plt.contour(X, Y, f(X, Y), 20, cmap='coolwarm');\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2UjeqZYQUJljTDsdp6m7h",
      "metadata": {},
      "source": [
        "The gradient $\\nabla f$ is obtained by differentiating $f$ with respect to the inputs $x$ and $y$\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "Uf2JXDGICqD_OApAGrgCx",
      "metadata": {},
      "source": [
        "def gradient_of_f(x, y):\n",
        "    return (-2*(x-2)*f(x, y), -2*(y+1)*f(x, y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "39iYqGZ3mhfecXb83d3JC",
      "metadata": {},
      "source": [
        "This gradient **ascent** algorithm (`gradient_ascent`\n",
        ")  seeks the values of $x$ and $y$ that give the **maximum** of $f(x, y)$.\n",
        "The inputs to `gradient_ascent`\n",
        " are the initial value of $x$ and $y$; a threshold that defines  the termination criterion for how small the gradient should be at the maximum; and a learning rate.\n",
        "The function returns the $x$ and $y$ values that maximize the function. For pedagogical purposes,  the function also returns a list called `history`\n",
        " of all the $x$-$y$ coordinates that were eastimated during the optimization procedure.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "wCl6RMkO051Afy3aPmL47",
      "metadata": {},
      "source": [
        "def gradient_ascent(x_init, y_init, \n",
        "                    threshold = 0.001,\n",
        "                    learningRate = 0.6):\n",
        "    x = x_init\n",
        "    y = y_init\n",
        "    history = [(x, y)]\n",
        "    done = False\n",
        "    while not done:        \n",
        "        gxy = gradient_of_f(x, y)\n",
        "        x += learningRate * gxy[0]\n",
        "        y += learningRate * gxy[1]\n",
        "        history.append([x, y])\n",
        "        if np.linalg.norm(gxy) < threshold:\n",
        "            done = True\n",
        "    return (x, y), history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "G4NJHYhDZRFQlORLkmB7v",
      "metadata": {},
      "source": [
        "As shown below, the algorithm gets fairly close to the global optimal value at $x$ and $y$ .\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "aSzmYBazLA53vQXEZLOK_",
      "metadata": {},
      "source": [
        "(x, y), history = gradient_ascent(0, 0)\n",
        "\n",
        "x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5wZgdwFUT10Pt_LsDu9NQ",
      "metadata": {},
      "source": [
        "The contour plot including the intermediate estimates of  (red points) of the gradient ascent algorithm as it climbs from (0, 0) towards the top (2, -1).\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "0chcTiLNLG9g0PqWSm9bM",
      "metadata": {},
      "source": [
        "# Make the contour plot.\n",
        "plt.contour(X, Y, f(X, Y), 20, cmap='coolwarm')\n",
        "\n",
        "# Plot the gradient ascent path.\n",
        "plt.plot([x for x, _ in history], [y for _, y in history], 'r.');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "L6p59wQErWA3_veRcEMKb",
      "metadata": {},
      "source": [
        "## Minimzing a Linear Least Squares using Gradient Descent\n",
        "This next example will show the **minimization** of a loss function using gradient descent.\n",
        "The objective function is the standard linear least squares:\n",
        "\n",
        "$$\n",
        "\\text{Loss}(w) = \\frac{1}{N} \\sum_{i=1}^N (\\theta^\\top x - y)^2\n",
        "$$\n",
        "\n",
        "Linear least squares regression has an _analytical_ (exact) solution that, as discussed ealier in the course,  can be found using the projection / pseudoinverse using the designmatrix $X$:\n",
        "\n",
        "$$\n",
        "\\theta = (X^T\\cdot X)^{-1} \\cdot X^T \\cdot y\n",
        "$$\n",
        "or SVD. \n",
        "However this example will use gradient descent for linear regression to exemplify\n",
        "- gradient descent on a simple and pedagogical example\n",
        "- that when the design matrix $X$ is very large it may become computationally intractable to compute the full solution using SVD/pseudo inverse. In fact this is the  reason that some of scikit-learn's  linear regression training algorithms (e.g. [`Ridge`\n",
        "](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
        ", [`Lasso`\n",
        "](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
        " etc) are based on various iterative optimization rather than using the exact solution.\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "vrSQRvQJgMVrdHyFtJBX7",
      "metadata": {},
      "source": [
        "class LeastSquaresRegressorGD():\n",
        "\n",
        "    def __init__(self, n_iter=20, tolerance=1e-5, eta=0.1):\n",
        "        self.n_iter = n_iter\n",
        "        self.tolerance = tolerance\n",
        "        self.eta = eta\n",
        "    def predict(self, X):\n",
        "        return X.dot(self.theta)  \n",
        "    \n",
        "    def fit(self, X, Y):\n",
        "        n_instances, n_features = X.shape\n",
        "        print(\"N-Features: %d\" % n_features)\n",
        "        self.theta = np.zeros(n_features)\n",
        "        print(\"Initial Theta %d\" % self.theta)\n",
        "        self.history = []\n",
        "        self.thetas = []\n",
        "        self.thetas.append(self.theta[0])\n",
        "        for i in range(self.n_iter):            \n",
        "           \n",
        "            # predictions on the whole training set using the current estimate theta\n",
        "            # this is a vector of predicted outputs\n",
        "            G = X.dot(self.theta)\n",
        "            \n",
        "            # vector of error values\n",
        "            Error = G - Y\n",
        "\n",
        "            # the loss value for the whole of the training set is the mean of the squared errors\n",
        "            total_loss = np.mean(Error**2)\n",
        "\n",
        "            # and the gradient is also computed over the whole training set, and the mean over the\n",
        "            # individual gradients computed.\n",
        "            gradient_theta = np.mean(2*Error*X.T, axis=1)\n",
        "\n",
        "            # if the gradient vector is small enough, terminate early\n",
        "            if np.linalg.norm(gradient_theta) <= self.tolerance:\n",
        "                break\n",
        "            \n",
        "            # update the weights in the direction of reducing the error for the whole training set. \n",
        "            self.theta -= self.eta*gradient_theta\n",
        "            self.thetas.append(self.theta[0])\n",
        "            self.history.append(total_loss)\n",
        "            \n",
        "        print('GD final loss (iteration {}): {:.4f}'.format(i+1, total_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "K0RsbEr8MgshhHpkxvnFB",
      "metadata": {},
      "source": [
        "### Testing the training algorithm on a synthetic dataset\n",
        "The example uses univariate data linear (one input feature - the slope) model with gaussian noise. \n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "0qODUSIl_Q6pF4t-dL4Sr",
      "metadata": {},
      "source": [
        "np.random.seed(0)\n",
        "numSamples = 1000\n",
        "slope = -0.9\n",
        "intercept = 0.4\n",
        "testsetSize= 400\n",
        "Xtrain = np.random.normal(size = (numSamples, 1))\n",
        "Ytrain = slope * Xtrain[:,0] + intercept * np.random.normal(size=Xtrain.shape[0])\n",
        "\n",
        "Xtest = np.sort(np.random.normal(size = (testsetSize, 1)))\n",
        "Ytest = slope * Xtest[:,0] + intercept * np.random.normal(size=Xtest.shape[0])\n",
        "\n",
        "plt.plot(Xtrain, Ytrain, 'g.');\n",
        "plt.plot(Xtest, Ytest, 'b.');\n",
        "\n",
        "plt.xlabel(\"Input feature\")\n",
        "plt.ylabel(\"Predicted output\")\n",
        "plt.title(\"Synthetic dataset\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "OEEZHLpM8DZWMx3zVueKl",
      "metadata": {},
      "source": [
        "You can play around with how the learning rate affects the convergence of the algorithm. If it's too low, convergence will be slow. If it's too high, convergence may also be slow (because the algorithm \"jumps around\") or it may not converge at all.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "-6_DYkK4ZqqLHFaJCqNIj",
      "metadata": {},
      "source": [
        "gd_regression = LeastSquaresRegressorGD(n_iter=20, eta=0.25, tolerance=0.005)\n",
        "gd_regression.fit(Xtrain, Ytrain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "k1pQPgmecL0CBEb2AQ8-n",
      "metadata": {},
      "source": [
        "The learning rate above is purposefully (eta) set quite low so that you below can see how the iterations will improve the fit. Setting the value to around $0.$ will have much faster convergence\n",
        "Plotting the `history`\n",
        "  during training reveals the progress of the optimization \n",
        "The gradient descent algorithm performs reasonably well (but poorly compared to more sophisticated models). That is unsurprising since the algortihm  has not made any updates by that point.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "4RvyIsvqVVafYEwCgc6yY",
      "metadata": {},
      "source": [
        "plt.plot(gd_regression.history, '.-')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "QPWoFaY8Rmf2JsNgkZAES",
      "metadata": {},
      "source": [
        "The evaluation of the MSE on the test set reveals that the model found by the training algorithm is reasonably good.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "obru48c30XaZoj6dl5V_V",
      "metadata": {},
      "source": [
        "print(mean_squared_error(Ytest, gd_regression.predict(Xtest)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "jguQ1TUsCJkKVvSKKU0x1",
      "metadata": {},
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "jwJ_QvN1dM1rarP14GmT8",
      "metadata": {},
      "source": [
        "for i, theta in enumerate(gd_regression.thetas):\n",
        "    # Show only the first 10 figures due\n",
        "    if(i>10):\n",
        "        break\n",
        "    gd_regression.theta = theta\n",
        "    y1 =  gd_regression.predict(np.array(-3))\n",
        "    y2 =  gd_regression.predict(np.array(3))\n",
        "    \n",
        "    plt.figure()\n",
        "\n",
        "    plt.plot(Xtrain, Ytrain, 'g.');\n",
        "    plt.plot(Xtest, Ytest, 'b.');\n",
        "\n",
        "    plt.xlabel(\"Input feature\")\n",
        "    plt.ylabel(\"Predicted output\")\n",
        "    plt.title(r\"Synthetic dataset (itr = %d, $\\theta=$ %2.4f)\" % (i,gd_regression.theta))\n",
        "    \n",
        "        \n",
        "    plt.plot([-3,3],[y1,y2],'r-')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "EFoczxu_JAd40sfNkZgTe",
      "metadata": {},
      "source": [
        ""
      ]
    }
  ]
}