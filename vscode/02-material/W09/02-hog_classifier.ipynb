{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "css_setup",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from IPython.core.display import HTML\n",
        "HTML(f\"\"\"\n",
        "<style>\n",
        "@import \"https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css\";\n",
        "</style>\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_HycWu_CJNJQjag6TJFHd",
      "metadata": {},
      "source": [
        "# HoG classifier\n",
        "The current exercise builds on the HOG exercises from the previous exercise\n",
        ", but it can also be successfully completed independently.\n",
        "The cell below imports the necessary libraries, loads an example image and extracts and plots the HOG features along the original image. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0MyPHoW4PC50cpdhpDmgc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from itertools import chain\n",
        "\n",
        "from skimage import data, transform, feature, exposure\n",
        "from skimage.feature import hog\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from skimage.color import rgb2gray\n",
        "from sklearn.feature_extraction.image import PatchExtractor\n",
        "from sklearn.svm import LinearSVC\n",
        "#from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "image = data.astronaut()\n",
        "\n",
        "fd, hog_image = hog(image, orientations=8, pixels_per_cell=(8, 8),\n",
        "                    cells_per_block=(2, 2), visualize=True, channel_axis=-1)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6), sharex=True, sharey=True)\n",
        "\n",
        "ax1.axis('off')\n",
        "ax1.imshow(image, cmap=plt.cm.gray)\n",
        "ax1.set_title('Input image')\n",
        "\n",
        "# Rescale histogram for better display\n",
        "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
        "\n",
        "ax2.axis('off')\n",
        "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
        "ax2.set_title('Histogram of Oriented Gradients')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nOm5IdE04ufN0BIvfA3G4",
      "metadata": {},
      "source": [
        "## Creating the training data\n",
        "Through the following steps you will construct the training data, which comprises of HOG features extracted from images containing faces and non-faces. The constructed set will be used for training a binary classification model.\n",
        "**Face Images (Positive class)**\n",
        "You will use the \"Labeled Faces in the Wild\" dataset (\"LFW\") for the images containing faces as provided by scikit-learn. The cell below loads the dataset, extracts the facial images, and displays a selection of these images for visualization. \n",
        "\n",
        "**Info**\n",
        "When first loading the dataset, please be aware that the code may require a few minutes to finish execution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rKRUiFJiA8UKX9c_bhFu5",
      "metadata": {},
      "outputs": [],
      "source": [
        "faces = fetch_lfw_people()\n",
        "\n",
        "face_images = faces.images\n",
        "print(face_images.shape) # 13233 face images to use for training\n",
        "\n",
        "# Visualize Positive Set (face images)\n",
        "\n",
        "fig, ax = plt.subplots(4,6)\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.imshow(face_images[500 * i], cmap='gray')\n",
        "    axi.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nT4VaP_D255Wpba4m8EyF",
      "metadata": {},
      "source": [
        "**Non-face Images (Negative class)**\n",
        "For non-face images other images from the Scikit-Image library are used. The cell below loads these images. The `PatchExtractor`\n",
        " method is used for data augmentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lGLY4G_I5qYmu2vVkkVr2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# negative data samples\n",
        "imgs_to_use = ['camera', 'text', 'coins', 'moon',\n",
        "               'page', 'clock', 'immunohistochemistry',\n",
        "               'chelsea', 'coffee', 'hubble_deep_field']\n",
        "# images = []\n",
        "\n",
        "images = [data.camera(), data.coins(),data.text(),data.moon(),data.page(),data.clock(),data.coffee(),data.hubble_deep_field()]\n",
        "img = []\n",
        "for im in images:\n",
        "    if len(im.shape)==3:\n",
        "        img.append(rgb2gray(im))\n",
        "    else:\n",
        "        img.append(im)\n",
        "\n",
        "### Make patches of the different negative samples to generate a larger dataset\n",
        "\n",
        "def extract_patches(img, N, scale=1.0, patch_size=face_images[0].shape):\n",
        "    extracted_patch_size = tuple((scale * np.array(patch_size)).astype(int))\n",
        "    extractor = PatchExtractor(patch_size=extracted_patch_size,\n",
        "                               max_patches=N, random_state=0)\n",
        "    patches = extractor.transform(img[np.newaxis])\n",
        "    if scale != 1:\n",
        "        patches = np.array([transform.resize(patch, patch_size)\n",
        "                            for patch in patches])\n",
        "    return patches\n",
        "\n",
        "non_face_images = np.vstack([extract_patches(im, 1000, scale)\n",
        "                              for im in img for scale in [0.5,1.0,2.0]]) # [0.5, 1.0, 2.0]\n",
        "print(non_face_images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_62e5rXrYQH-4I2dtadpm",
      "metadata": {},
      "source": [
        "The cell below visualizes the newly created non-face image patches:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BWmF9ObruGFDoPR68pc7i",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Negative Set\n",
        "\n",
        "fig, ax = plt.subplots(4,6)\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.imshow(non_face_images[600 * i], cmap='gray')\n",
        "    axi.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kxoIa84VXtXNZcnDS4kHp",
      "metadata": {},
      "source": [
        "The cell below combines the face and the non face images, then extracts the hog features from the combined set. It also creates the target labels for the newly assembled training dataset. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DIYWDp-vTLYkHtKM5KdE9",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = np.array([hog(im, orientations=8, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=False)\n",
        "                    for im in chain(face_images, non_face_images)])\n",
        "\n",
        "\n",
        "Y_train = np.zeros(X_train.shape[0])\n",
        "Y_train[:face_images.shape[0]] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kxHu5YFKPv4bVXlXCe7cX",
      "metadata": {},
      "source": [
        "## Training a classifier\n",
        "In the following task you will train a linear classifier using HoG features.\n",
        "\n",
        "---\n",
        "**Task 1 (easy): Implement a classifierüë©‚Äçüíª**\n",
        "The cell below contains the classification model. Your task is to:\n",
        "1. Fit the model to the training data.\n",
        "\n",
        "2. Calculate and print the accuracy of the classifier on the training data.\n",
        "\n",
        "3. Construct and plot a confusion matrix of the model predictions on the training set.\n",
        "\n",
        "4. Based on the accuracy and the confusion matrix, discuss how effective the model is. \n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nylCOZKVLpYd05x_XR5X_",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
        "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
        "           verbose=0)\n",
        "\n",
        "# implement model from scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1d2OPF_hyS_b0GEuajRG",
      "metadata": {},
      "source": [
        "## Testing on new images\n",
        "In this section you will evaluate the trained model on a novel image. You will implement a function that slides over the image and breaks it down to smaller patches. Then you will extract the HOG features from these patches, and categorize them as either a face (1) or a non-face (0) image. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YWpG54xiabt_hmdY7hbLg",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a test image\n",
        "test_img = data.astronaut()\n",
        "test_img = rgb2gray(test_img)\n",
        "test_img = transform.rescale(test_img, 0.5)\n",
        "test_img = test_img[:120, 60:160]\n",
        "\n",
        "\n",
        "plt.imshow(test_img, cmap='gray')\n",
        "plt.axis('off')\n",
        "print(test_img.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VxfLbQ-mBYbZP0RM_8Yr2",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "**Task 2 (hard): Sliding windowüë©‚Äçüíª**\n",
        "1. Implement the method `sliding_windows` that takes the test image and extracts image patches from it. \n",
        "    - The function should iteratively slide over the entire image and extract image patches of the same size as the inputs the classifier was trained on.\n",
        "    - The function should return a tuple of the coordinates of the upper left corner of each subimage patch, and the patch itself.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uyvNROSZkpx4yktzbycWA",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sliding Window function - Goes Over the image patch by patch\n",
        "# and computes the HOG features for each patch.\n",
        "\n",
        "def sliding_window(img, patch_size=face_images[0].shape, istep=, jstep=):\n",
        "    indicies = []\n",
        "    patches = []\n",
        "    ...\n",
        "    return indicies, patches\n",
        "\n",
        "# Write you code iterating over the test image here.\n",
        "# EASIEST approach is to use a foor loop for each image dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZWqapHkS53jPCy5o8YuU4",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "**Task 3 (medium): Classifying the test imageüë©‚Äçüíª**\n",
        "In the following task you have to:\n",
        "1. Extract the HOG features for all patches from the test image (`test_img`) by making use of the `sliding_window` and `hog` functions. Store the features in an array called `hog_patches` and the corresponding indices in an array called `indices`.\n",
        "\n",
        "2. Use the model to classify the extracted HOG patches and store the results in an array called `labels`\n",
        ".\n",
        "\n",
        "3. Use the existing code to visualize the detections. \n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pNze22G6oY43aJeYBDirk",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply sliding window function to test_img\n",
        "\n",
        "# Write your code here\n",
        "\n",
        "\n",
        "# Visualize the detections\n",
        "fig, ax = plt.subplots()\n",
        "ax.imshow(test_img, cmap='gray')\n",
        "ax.axis('off')\n",
        "\n",
        "Ni, Nj = face_images[0].shape\n",
        "indices = np.array(indices)\n",
        "\n",
        "for i, j in indices[labels == 1]:\n",
        "    ax.add_patch(plt.Rectangle((j, i), Nj, Ni, edgecolor='red', alpha=0.3, lw=2, facecolor='none'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CYICAhS-JxLzGk-4QA_3R",
      "metadata": {},
      "source": [
        "## Reflections\n",
        "\n",
        "---\n",
        "**Task 4 (easy): Reflectionsüí°**\n",
        "- Summarize the overall process of face detection implemented in this exercise. \n",
        "- There are multiple overlapping boxes of the same detected face in the test image. How could you ensure that a face is only detected once? \n",
        "- How could the face detection pipeline be improved? Consider the following steps: training data construction, model fitting, test images.\n",
        "\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": "py",
      "mimetype": "text/x-python",
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
