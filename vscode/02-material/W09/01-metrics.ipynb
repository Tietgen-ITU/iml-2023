{
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": "py",
      "mimetype": "text/x-python",
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "cell_type": "code",
      "id": "css_setup",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        }
      },
      "source": [
        "import requests\n",
        "from IPython.core.display import HTML\n",
        "HTML(f\"\"\"\n",
        "<style>\n",
        "@import \"https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css\";\n",
        "</style>\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4tJbPdQwIrCkogiUc0rFn",
      "metadata": {},
      "source": [
        "# Evaluating Classifiers\n",
        "Throughout these exercises, you will implement and design evaluation metrics for classification models. The metrics used are provided by the scikit-learn library. The default metric for evaluating a classification model is `accuracy`\n",
        ", but Scikit-learn also has a `metrics`\n",
        " module that provides a variety of metrics serving different purposes.\n",
        "The cell below imports the necessary libraries and sets the default parameters.\n",
        "\n",
        "**Info**\n",
        "Your main focus is to evaluate classification models so you don't need to concern yourself with the intricacies of the actual models.\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "X1l2U1L2-o5cQ1vnK0E3M",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_curve, auc, average_precision_score, roc_curve\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "V1RbyLZcl4segpgiQzCQO",
      "metadata": {},
      "source": [
        "## Classification Metrics:\n",
        "Run the cell below to generate a classification dataset comprising of 500 samples, 20 features, and 2 distinct classes. This dataset is partitioned using an 80-20 train-test split.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "rezHrexeChKglJYrJ2K0_",
      "metadata": {},
      "source": [
        "X,Y  = datasets.make_classification(n_samples=500, n_features=20, n_classes=2, random_state=1)\n",
        "print('Dataset Size : ',X.shape,Y.shape)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.80, test_size=0.20, stratify=Y, random_state=1)\n",
        "print('Train/Test Size : ', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "m9oveGY6XblzE4ENLZHSO",
      "metadata": {},
      "source": [
        "### Classification Accuracy\n",
        "In the following task you will evaluate the model's performance on the test data using different metrics. Run the cell below to train the first classification model. \n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "lqenBtnU5YqoKdPsV0Z52",
      "metadata": {},
      "source": [
        "classifier1 = LinearSVC(random_state=1, C=0.1)\n",
        "classifier1.fit(X_train, Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5C5BnIABFJ0pk_z0f9QFp",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "**Task 1 (easy): Model accuracy👩‍💻**\n",
        "Classification accuracy is the number of true predictions divided by the total number of samples. You can use the `score`\n",
        " function to obtain the accuracy, or you can use the `predict`\n",
        " function on either set and then calculate the average accuracy by comparing the predicted labels to the true labels. Choose one of these methods to: \n",
        "1. Calculate model accuracy on the training set.\n",
        "2. Calculate model accuracy on the testing set.\n",
        "3. Construct and plot a confusion matrix of the model predictions on the testing set. \n",
        "\n",
        "\n",
        "**Info**\n",
        "Recall that the confusion matrix for binary classification problems has the following structure:\n",
        "\n",
        "$$\n",
        "\\begin{array}{cc|c|c|}\n",
        "  & & \\text{Predicted Negative} & \\text{Predicted Positive} \\\\\n",
        "\\hline\n",
        "\\text{Actual Negative} & & TN & FP \\\\\n",
        "\\hline\n",
        "\\text{Actual Positive} & & FN & TP \\\\\n",
        "\\end{array}\n",
        "$$\n",
        "- **TN** - True Negative\n",
        "- **FN** - False Negative \n",
        "- **FP** - False positive\n",
        "- **TP** - True Positive\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "HG_oiFVhN7_8xsLSQJb2v",
      "metadata": {},
      "source": [
        "# write your code here\n",
        "\n",
        "\n",
        "# write your solution here ... \n",
        "sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "j8yAFNeGVXo0Zs4QlMY37",
      "metadata": {},
      "source": [
        "### Classification Report\n",
        "The classification report provides a more detailed overview of the model's classification performance. It includes metrics such as precision, recall, f1-score, and specificity.\n",
        "- **Precision** - or positive predictive value, represents how many predictions of the positive class actually belong to that class. \n",
        "\n",
        "$$\n",
        "\\frac{𝑇𝑃}{𝑇𝑃+𝐹𝑃}\n",
        "$$\n",
        "\n",
        "\n",
        "- **Recall** -  also known as sensitivity, true positive rate, or hit rate and assesses whether classifier correctly identifies positive instances out of the total actual postive instances. \n",
        "\n",
        "$$\n",
        "\\frac{𝑇𝑃}{𝑇𝑃+𝐹𝑁} \n",
        "$$\n",
        "\n",
        "\n",
        "- **F1-score** - harmonic mean of precision & recall. \n",
        "\n",
        "$$\n",
        "2∗\\frac{𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛∗𝑟𝑒𝑐𝑎𝑙𝑙}{𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+𝑟𝑒𝑐𝑎𝑙𝑙} \n",
        "$$\n",
        "\n",
        "\n",
        "- **Specificity** - also known as the True Negative Rate, is the percentage of correctly predicted instances of the negative class \n",
        "\n",
        "$$\n",
        "\\frac{TN}{TN+FP} \n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**Task 2 (easy): Classification report👩‍💻💡**\n",
        "1. Use the confusion matrix from Task 1 to obtain the true positive, false positive, true negative and false negative values. \n",
        "\n",
        "2. Use the obtained values to implement the above formulas and calculate:\n",
        "\n",
        "\n",
        "- Precision\n",
        "- Recall\n",
        "- F1-score\n",
        "- Specificity\n",
        "\n",
        "3. Inspect the metrics and reflect on theier significance, especially when dealing with class imbalance in your dataset. How do these metrics help in understanding and evaluating the performance of a classification model in situations where one class significantly outnumbers the other?\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "vJo6W8UiMCIYNcD3GyOH1",
      "metadata": {},
      "source": [
        "# write your solution here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "y9A-JTsrhh685mPhz41qh",
      "metadata": {},
      "source": [
        "## Imbalanced classes\n",
        "We will generate a new dataset comprising 1000 samples distributed across 10 classes, but we intentionally create an imbalance. In the cell below, the imbalance is created by marking all samples belonging to the positive class (0) as True, and all the remaining classes as False. As a result, 10% of values belong to the positive class and the remaining 90% to the negative class.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "CnMvAAmy-DSmldhVm4W8X",
      "metadata": {},
      "source": [
        "X, Y = datasets.make_classification(n_samples=1000, n_classes=10, n_informative=10)\n",
        "\n",
        "# Mark the minority class as True\n",
        "Y = (Y == 0).astype(int)\n",
        "\n",
        "print('Dataset Size:', X.shape, Y.shape)\n",
        "\n",
        "# Check the imbalance ratio\n",
        "imbalance_ratio_actual = np.mean(Y)\n",
        "print(f'Imbalance Ratio (Positive/Minority Class): {imbalance_ratio_actual:.2f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "-SMpZ75qtkysKUDHPeGp9",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "**Task 3 (hard): Class Imbalance👩‍💻💡**\n",
        "Throughout the following task you will implement and evaluate a second classifier on the imbalanced dataset:\n",
        "1. The for loop below performs the data splitting for 5-fold cross-validation. Complete the loop to: \n",
        "\n",
        "- Fit the second classification model on the training folds \n",
        "- Predict the labels on the validation folds\n",
        "- Calculate model accuracy on the validation folds\n",
        "\n",
        "2. Plot the accuracy of the model on each fold. What does the plot tell you about classification performance?\n",
        "3. Extend your loop to obtain the classification report on the testing set (precision, recall, F1-Score, specificity). Print the average of each metric (use `np.nanmean()`\n",
        " as some of these metrics might inlcude NaNs)\n",
        "\n",
        "\n",
        "**Hint**\n",
        "First construct the confusion matrix.\n",
        "\n",
        "4. Is the model able to reliably identify the minority class? What are the implications for the model's performance and its practical utility?\n",
        "5. (Optional) Plot all the metrics in the same plot. \n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "Rr1nsLOGluG3-Fmq2UGc7",
      "metadata": {},
      "source": [
        "classifier2 = SVC()\n",
        "\n",
        "accuracies= []\n",
        "accuracies, precisions, recalls, f1_scores, specificities = [], [], [], [], []\n",
        "\n",
        "for train_idx_svc, test_idx_svc in KFold(n_splits=5, shuffle=True).split(X):\n",
        "    X_train, X_test = X[train_idx_svc], X[test_idx_svc]\n",
        "    Y_train, Y_test = Y[train_idx_svc], Y[test_idx_svc]\n",
        "\n",
        "    # write your solution here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "YQ-5LUTISl5W-_jsWhW5z",
      "metadata": {},
      "source": [
        "## ROC Curves and Precision-Recall Curves\n",
        "The ROC (Receiver Operating Characteristic) Curve is a valuable tool for assessing model performance, particularly in binary classification tasks. The Precision-Recall Curve is a graphical representation used to evaluate the performance of a classification model, particularly in situations involving class imbalance or when the positive class is of greater interest. \n",
        "In the following task you will examine the ROC curve and the Precision-Recall curve for the second classifier, which you previously trained on the imbalanced dataset. \n",
        "\n",
        "---\n",
        "**Task 4 (medium): ROC and Precision-Recall curves👩‍💻💡**\n",
        "Run the cell below to ensure that the appropriate proportions of the imbalanced dataset are allocated to the training and testing sets, respectively. The provided code also calculates both the ROC (Receiver Operating Characteristic) curve and the Precision-Recall curve and computes relevant metrics. These metrics include: `fpr`\n",
        " (False Positive Rate), `tpr`\n",
        " (True Positive Rate, also known as `recall`\n",
        "), and `precision`\n",
        ".\n",
        "Complete the code to:\n",
        "1. Plot the ROC curve.\n",
        "2. Plot the Precision-Recall curve.\n",
        "3. How do these scores provide insights into a model's ability to handle class imbalance? \n",
        "4. Describe when and why you would prioritize one curve over the other when dealing with imbalanced data.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "KAcAvb6EQ5nk9-B_yiAgN",
      "metadata": {},
      "source": [
        "X_train, X_test = X[train_idx_svc], X[test_idx_svc]\n",
        "Y_train, Y_test = Y[train_idx_svc], Y[test_idx_svc]\n",
        "\n",
        "# ROC curve\n",
        "decision_function = classifier2.decision_function(X_test)\n",
        "fpr, tpr, _ = roc_curve(Y_test, decision_function)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Calculate Precision-Recall curve\n",
        "precision, recall, _ = precision_recall_curve(Y_test, decision_function)\n",
        "pr_auc = average_precision_score(Y_test, decision_function)\n",
        "\n",
        "# write your solution here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "LaYZGJ2qK4sGOF0ZcQCTL",
      "metadata": {},
      "source": [
        ""
      ]
    }
  ]
}