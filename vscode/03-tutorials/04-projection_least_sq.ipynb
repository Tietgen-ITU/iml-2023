{
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": "py",
      "mimetype": "text/x-python",
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": {
          "outputs_hidden": null,
          "source_hidden": true
        },
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "import requests\n",
        "from IPython.core.display import HTML\n",
        "HTML(f\"\"\"\n",
        "<style>\n",
        "@import \"https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css\";\n",
        "</style>\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "# Projection and Least Squares Tutorial\n",
        "This tutorial will guide you through the basics of linear projections and their relation to least squares.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "## Projections\n",
        "Recall from the reading material that an orthogonal projection is a transformation that maps vectors onto a subspace in such that the distances between original and projected points are minimal. \n",
        "**Example**\n",
        "Define a set of points $X=\\begin{bmatrix}|&&|\\\\x_1&\\dots&x_n\\\\|&&|\\end{bmatrix} \\in \\mathbb{R}^{2\\times n}$ (`points`\n",
        " in the code) and a line $\\mathcal{l}$ defined by the function $f(x)=0.5x$. The task is to project $X$ onto $\\mathcal{l}$:\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "# Three points\n",
        "X = np.array([\n",
        "    [1, 2],\n",
        "    [2, 1.5],\n",
        "    [3, 1.2]\n",
        "]).T\n",
        "\n",
        "# Show plot\n",
        "plt.scatter(X[0, :], X[1, :], c=\"r\")\n",
        "\n",
        "# Make line points (remember Numpy broadcasting)\n",
        "x = np.linspace(0, 4)\n",
        "f_x = x * 0.5\n",
        "\n",
        "# Plot line\n",
        "plt.plot(x, f_x);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "A point $x_i$ is projected onto the line $\\mathcal{l}$ by multiplying $Px_i$ where $P$ is the projection matrix. Projecting all points in $X$ onto $l$ is therefore $X^{\\prime}=PX$.\n",
        "The projection matrix $P$ is given by:\n",
        "\n",
        "$$\n",
        "P = A(A^TA)^{-1}A^T,\n",
        "$$\n",
        "\n",
        "where $A$ is the _design matrix_. $A=\\begin{bmatrix}1\\\\0.5\\end{bmatrix}$ for the line $l$, i.e., $\\begin{bmatrix}x\\\\y\\end{bmatrix} = At$. \n",
        "\n",
        "**Info**\n",
        "- The columns of $A$ span the subspace we want to project the point $x^{\\prime}_i$ onto.\n",
        "- The design matrix for $l$ is one-dimensional \n",
        "\n",
        "\n",
        "\n",
        "The code cell below calculates $P$ and projects $X$ onto $l$:\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "##1\n",
        "#The line l written as the design matrix\n",
        "A = np.array([[1, 0.5]]).T  #  has to be a column vector\n",
        "\n",
        "##2\n",
        "## construct projection matrix\n",
        "P = (A @ np.linalg.inv(A.T @ A)) @ A.T\n",
        "print(\"P:\\n\", P)\n",
        "\n",
        "#projection the points with matrix multiplication\n",
        "x_prime = P @ X\n",
        "print(\"projected points:\\n\", x_prime)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "The projection process is visualized below:\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "# Creating a square figure (makes it easier to visually confirm projection)\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "plt.scatter(X[0, :], X[1, :], label=\"Original points\")  # Old points\n",
        "plt.scatter(x_prime[0, :], x_prime[1, :], label=\"Projected points\")  # Projected points\n",
        "plt.plot(x, f_x, label=\"Line\")  # Line\n",
        "plt.legend()\n",
        "\n",
        "# Gather old and projected points in a single array\n",
        "P1 = np.concatenate([X.T[:, :].reshape(1, 3, 2), x_prime.T[:, :].reshape(1, 3, 2)], axis=0)\n",
        "# Plot projection/error lines\n",
        "plt.plot(P1[:, 0, 0], P1[:, 0, 1], 'g--')\n",
        "plt.plot(P1[:, 1, 0], P1[:, 1, 1], 'g--')\n",
        "plt.plot(P1[:, 2, 0], P1[:, 2, 1], 'g--')\n",
        "\n",
        "# Set axes limits to be the same for equal aspect ratio\n",
        "plt.xlim(0, 3.5)\n",
        "plt.ylim(0, 3.5);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "\n",
        "**Observe**\n",
        "The projection lines (dashed lines) and $\\mathcal{l}$ are orthogonal. \n",
        "\n",
        "\n",
        "## Linear Least Squares\n",
        "This exercise is about fitting a straight line (model) to a set of points (matrix $X$). The goal is to find the line that minimizes the error between the actual points and the points predicted by the model. \n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "## 3\n",
        "# Define the example points\n",
        "X = np.array([\n",
        "    [1, 1],\n",
        "    [2, 2],\n",
        "    [3, 2]\n",
        "]).T\n",
        "\n",
        "plt.scatter(X[0, :], X[1, :]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "In the previous section, the set of points were projected onto an existing line. In this section, projections are used to perform linear least squares to find model parameters. The model is $f_\\mathbf{w}(x) = y = \\mathbf{w}_1x + \\mathbf{w}_2$, where $x$ is the input and $\\mathbf{w}_1$, $\\mathbf{w}_2$ are the parameters. The model can be expressed as an inner product $f_\\mathbf{w}(x) = y = \\begin{bmatrix}x& 1\\end{bmatrix}\\mathbf{w}$.\n",
        "With multiple points, a linear set of equations is given by\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}x_1 & 1\\\\\\vdots & \\vdots \\\\x_n&1\\end{bmatrix} \\mathbf{w} = A\\mathbf{w} = \\mathbf{y} = \\begin{bmatrix}y_1\\\\ \\vdots \\\\y_n\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Two points are necessary to solve for the model parameters using inverses ($A$ is square). When the set of points $X$ contains more than two points, a linear least squares solution for $\\mathbf{w}$ is needed. \n",
        "\n",
        "**Info**\n",
        "Try to visualize placing one line in the figure above that intersects all points. This is clearly impossible! However, if the points $X$ were all on the same line, an exact solution could be found since the rows of $X$ are linearly dependent ($X$ can be rewritten as a square matrix and as a result, $A$ becomes square). \n",
        "\n",
        " \n",
        "A least squares solution implies that there will be some error between  $\\mathbf{y}$ and the predicted points $\\mathbf{\\hat{y}}=A\\mathbf{w}$. Additionally, $\\mathbf{\\hat{y}}$ must be in the span of $A$ since it is a linear combination of its column vectors. This is visualized for two dimensions in [Figure 1](#lstsq). Observe that $\\mathbf{\\hat{y}}$ in the figure must be on the line spanned by $A$.\n",
        "<figure class=\"figure\" id=\"lstsq\">\n",
        "\n",
        "<img src=\"/resources/material/W04/lstsq.png\" class=\"figure-img img-fluid rounded\" style=\"width:50%\" />\n",
        "\n",
        "<figcaption class=\"figure-caption has-text-centered\">Figure 1: Least squares demonstrations using a two-dimensional design matrix. It visualizes how $\\mathbf{\\hat{y} }$ is the projection of $\\mathbf{y}$ onto $A$ and that $\\mathbf{\\hat{y} }=A\\mathbf{w}$. It also shows the error vector $e$.</figcaption>\n",
        "</figure>\n",
        "\n",
        "Minimizing the error is done by projecting $\\mathbb{y}$ onto $A$ as demonstrated for two dimensions in [Figure 1](#lstsq). This reveals the relation $\\mathbf{\\hat{y}} = P\\mathbf{y}$ and hence $A\\mathbf{w} = P \\mathbf{y}=A(A^\\top A)^{-1}A^\\top \\mathbf{y}$. \n",
        "The final step is solving for $\\mathbf{w}$. Since $A$ is multiplied on both sides, isolating $\\mathbf{w}$ yields:\n",
        "\n",
        "$$\n",
        "\\mathbf{w} = (A^\\top A)^{-1}A^\\top \\mathbf{y}.\n",
        "$$\n",
        "\n",
        "\n",
        "**Derivation (optional)**\n",
        "The exact derivation requires a few extra steps and is not expected knowledge for the exam. However, we include it here for those who want to convince themselves that the solution for $\\mathbf{w}$ is indeed correct:\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\mathbf{\\hat{y}} & = P\\mathbf{y} &\\\\\n",
        "A\\mathbf{w} & = A(A^TA)^{-1}A^T \\mathbf{y} & \\text{Substitute }\\mathbf{\\hat{y}}=A\\mathbf{w}\\text{ and }P=A(A^TA)^{-1}A^T\\\\\n",
        "(A^TA)^{-1}A^TA\\mathbf{w} & = (A^TA)^{-1}A^TA(A^TA)^{-1}A^T \\mathbf{y} & \\text{Multiply both sides by }(A^TA)^{-1}\\\\\n",
        "\\mathbf{w} & = (A^TA)^{-1}A^T \\mathbf{y} & \\text{Simplify using the fact that }(A^TA)^{-1}A^TA=I\\\\\n",
        "\\end{align*}$$\n",
        "\n",
        "\n",
        "\n",
        "### Design matrix\n",
        "The design matrix $A$:\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "x_vals = X[0, :]\n",
        "y_vals = X[1, :]\n",
        "\n",
        "A = np.vstack((x_vals, np.ones(x_vals.shape))).T\n",
        "print(\"A\\n\", A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "### Projection\n",
        "The following cell implements the solution for $\\mathbf{w}$ described above:\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "P = np.linalg.inv(A.T @ A) @ A.T\n",
        "# Applying the transformation\n",
        "w = P @ y_vals\n",
        "print(\"w:\", w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "To get the predicted points $\\mathbf{\\hat{y}}$, $A$ is multiplied onto the parameter vector:\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "# Calculating the projected y-values\n",
        "y_hat = A @ w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "The `w`\n",
        " vector is of the form $(a, b)$ and the line formula is $f(x)=ax+b$. Below, we calculate a number of points on the line for visualization purposes and compare with both the original and projected points:\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "x = np.linspace(0, 5)  # Create range of values\n",
        "y = x * w[0] + w[1]  # Calculate f(x)\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.plot(x, y)  # Plot line\n",
        "plt.scatter(X[0, :], X[1, :])  # Plot original points\n",
        "\n",
        "plt.scatter(X[0, :], y_hat)  # Plot the points\n",
        "plt.title('Least squares linear regression 3 points')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "### Measuring the error\n",
        "Remember that both $\\mathbf{y}$ and $\\mathbf{\\hat{y}}$ are vectors. The projection error is:\n",
        "\n",
        "$$\n",
        "e = \\|\\mathbf{y}-\\mathbf{\\hat{y}}\\| = \\sqrt{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2} = \\sqrt{(\\mathbf{y}-\\mathbf{\\hat{y}})(\\mathbf{y}-\\mathbf{\\hat{y}})^\\top}.\n",
        "$$\n",
        " \n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "# Calculating the error\n",
        "diff = y_vals - y_hat\n",
        "e = np.sqrt(diff @ diff.T)\n",
        "print(\"e\", e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "To get the mean error, we use\n",
        "\n",
        "$$\n",
        "RMS(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2} = \\sqrt{\\frac{1}{n}(\\mathbf{y}-\\mathbf{\\hat{y}})(\\mathbf{y}-\\mathbf{\\hat{y}})^\\top}.\n",
        "$$\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": null,
        "autoscroll": null,
        "deletable": null,
        "jupyter": null,
        "format": null,
        "name": null,
        "tags": null
      },
      "source": [
        "diff = y_vals - y_hat\n",
        "rms = np.sqrt((diff @ diff.T).mean())\n",
        "print(\"root mean squared error\", rms)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}