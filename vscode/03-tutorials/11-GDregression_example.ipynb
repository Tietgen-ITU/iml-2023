{
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": "py",
      "mimetype": "text/x-python",
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "cell_type": "code",
      "id": "css_setup",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        }
      },
      "source": [
        "import requests\n",
        "from IPython.core.display import HTML\n",
        "HTML(f\"\"\"\n",
        "<style>\n",
        "@import \"https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css\";\n",
        "</style>\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4MNCHCThVh7Q7nrCawx8z",
      "metadata": {},
      "source": [
        "# Gradient Descent / Ascent\n",
        "This tutorial is for illustrating the plain vanilla gradient descent algorithm. While this is valuable for educational purposes, it's common in real-world applications to utilize advanced libraries like PyTorch or scikit-learn, which offer more refined and efficient implementations.\n",
        "## Gradient Ascent on a function\n",
        "This example shows a basic implementation of the gradient ascent algorithm.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "CWbjEoS1HmXJ-Aj2BMXit",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "plt.style.use('bmh')\n",
        "from IPython.core.display import HTML as Center\n",
        "import itertools as itr\n",
        "\n",
        "Center(\"\"\" <style>\n",
        ".output_png {\n",
        "    display: table-cell;\n",
        "    text-align: center;\n",
        "    vertical-align: middle;\n",
        "}\n",
        "</style> \"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "dEo1_0vlLYpQ6XUthunIW",
      "metadata": {},
      "source": [
        "Define a bivariate (two-variables) function, $f(x,y)=\\exp(-(x-2)^2 - (y+1)^2)$, to **maximize**. The true **maximum** is easily seen to be at $x=2$ and $y=-1$.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "BEgC5oOuH9GVtMDf6OHiy",
      "metadata": {},
      "source": [
        "def f(x, y):\n",
        "    return np.exp(-(x-2)**2 - (y+1)**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "kg-jdHfeiSPg8AUOuOiXg",
      "metadata": {},
      "source": [
        "The surface plot is intended to offer a visual representation of the function's behavior and characteristics.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "fwg7525-oEkB4ZYTdpc5L",
      "metadata": {},
      "source": [
        "# The ranges of x and y values that we'd like to include in the contour plot.\n",
        "x_range = np.linspace(-1, 5, 40)\n",
        "y_range = np.linspace(-3, 3, 40)\n",
        "\n",
        "# Form all possible x-y pairs in these ranges.\n",
        "X, Y = np.meshgrid(x_range, y_range)\n",
        "\n",
        "# Make the surface plot.\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X, Y, f(X, Y), cmap='coolwarm')\n",
        "\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "#ax.set_zlabel('f(x, y)')\n",
        "ax.set_title(r'$f(x,y)=\\exp(-(x-2)^2 - (y+1)^2)$');\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "xfgeE7ifVmiWG26D39DGC",
      "metadata": {},
      "source": [
        "The corresponding contour plot where darker rings represent lower values and lighter rings indicate higher values,  is shown below\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "b_UfX6CcivsEjtFek8NBW",
      "metadata": {},
      "source": [
        "plt.contour(X, Y, f(X, Y), 20, cmap='coolwarm');\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "hnUEr350J240Dfl7VDC43",
      "metadata": {},
      "source": [
        "The gradient $\\nabla f$ is obtained by differentiating $f$ with respect to the inputs $x$ and $y$\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "9N_19dRYGQNw9mM6EUz_T",
      "metadata": {},
      "source": [
        "def gradient_of_f(x, y):\n",
        "    return (-2*(x-2)*f(x, y), -2*(y+1)*f(x, y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "zsCoNhtc0Glzg9tLrb8XM",
      "metadata": {},
      "source": [
        "This gradient **ascent** algorithm (`gradient_ascent`\n",
        ")  seeks the values of $x$ and $y$ that give the **maximum** of $f(x, y)$.\n",
        "The inputs to `gradient_ascent`\n",
        " are the initial value of $x$ and $y$; a threshold that defines  the termination criterion for how small the gradient should be at the maximum; and a learning rate.\n",
        "The function returns the $x$ and $y$ values that maximize the function. Additionally, for educational reasons, it provides a history list containing all the estimated \n",
        "$x$ and $y$ points evaluated throughout the optimization process.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "ilKgkxUiTaQR3sRIr-0Ib",
      "metadata": {},
      "source": [
        "def gradient_ascent(x_init, y_init, \n",
        "                    threshold = 0.001,\n",
        "                    learningRate = 0.6):\n",
        "    x = x_init\n",
        "    y = y_init\n",
        "    history = [(x, y)]\n",
        "    done = False\n",
        "    while not done:        \n",
        "        gxy = gradient_of_f(x, y)\n",
        "        x += learningRate * gxy[0]\n",
        "        y += learningRate * gxy[1]\n",
        "        history.append([x, y])\n",
        "        if np.linalg.norm(gxy) < threshold:\n",
        "            done = True\n",
        "    return (x, y), history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "-E8sv9vZwO1cdMiMXojOB",
      "metadata": {},
      "source": [
        "The results presented below demonstrate that the algorithm approaches the global optimal values at $x$ and $y$ quite closely.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "y0hWejXTJ023DYQoIH3la",
      "metadata": {},
      "source": [
        "(x, y), history = gradient_ascent(0, 0)\n",
        "\n",
        "x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f4iOB5OyoyoC6V6LptO_8",
      "metadata": {},
      "source": [
        "The contour plot including the intermediate estimates of  (red points) of the gradient ascent algorithm as it climbs from (0, 0) towards the top (2, -1).\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "pUSGeAxJxpW19-F3NrOwu",
      "metadata": {},
      "source": [
        "# Make the contour plot.\n",
        "plt.contour(X, Y, f(X, Y), 20, cmap='coolwarm')\n",
        "\n",
        "# Plot the gradient ascent path.\n",
        "plt.plot([x for x, _ in history], [y for _, y in history], 'rx');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ZkvoNaBp7_YG7Bu9PH-DW",
      "metadata": {},
      "source": [
        "## Minimzing a Linear Least Squares using Gradient Descent\n",
        "This next example will show the **minimization** of a loss function using gradient descent.\n",
        "The objective function is the standard linear least squares:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w) = \\frac{1}{N} \\sum_{i=1}^N (w^\\top x - y)^2\n",
        "$$\n",
        "\n",
        "Linear least squares regression has an _analytical_ (exact) solution that, as discussed ealier in the course,  can be found using the projection / pseudoinverse using the designmatrix $A$:\n",
        "\n",
        "$$\n",
        "w = (A^\\top\\cdot A)^{-1} \\cdot A^\\top \\cdot y\n",
        "$$\n",
        "However this example will use gradient descent for linear regression to exemplify\n",
        "- gradient descent on a simple and pedagogical example\n",
        "- that when the design matrix $A$ is very large it may become computationally intractable to compute the full solution using pseudo inverse. In fact this is the  reason that some of scikit-learn's  linear regression training algorithms (e.g. [`Ridge`\n",
        "](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
        ", [`Lasso`\n",
        "](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
        " etc) are based on iterative optimization rather than using the exact solution.\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "Fy2gRQtiuOF2nVBonp6bm",
      "metadata": {},
      "source": [
        "class LeastSquaresRegressorGD():\n",
        "\n",
        "    def __init__(self, n_iter=20, tolerance=1e-5, learningRate=0.1):\n",
        "        self.n_iter = n_iter\n",
        "        self.tolerance = tolerance\n",
        "        self.learningRate = learningRate\n",
        "    def predict(self, X):\n",
        "        return X.dot(self.w)  \n",
        "    \n",
        "    def fit(self, X, Y):\n",
        "        n_instances, n_features = X.shape\n",
        "        print(\"N-Features: %d\" % n_features)\n",
        "        self.w = np.zeros(n_features)\n",
        "        print(\"Initial w %d\" % self.w)\n",
        "        self.history = []\n",
        "        self.ws = []\n",
        "        self.ws.append(self.w[0])\n",
        "        for i in range(self.n_iter):            \n",
        "           \n",
        "            # predictions on the whole training set using the current estimate theta\n",
        "            # this is a vector of predicted outputs\n",
        "            G = X.dot(self.w)\n",
        "            \n",
        "            # vector of error values\n",
        "            Error = G - Y\n",
        "\n",
        "            # the loss value for the whole of the training set is the mean of the squared errors\n",
        "            total_loss = np.mean(Error**2)\n",
        "\n",
        "            # and the gradient is also computed over the whole training set, and the mean over the\n",
        "            # individual gradients computed.\n",
        "            gradient_w = np.mean(2*Error*X.T, axis=1)\n",
        "\n",
        "            # if the gradient vector is small enough, terminate early\n",
        "            if np.linalg.norm(gradient_w) <= self.tolerance:\n",
        "                break\n",
        "            \n",
        "            # update the weights in the direction of reducing the error for the whole training set. \n",
        "            self.w -= self.learningRate*gradient_w\n",
        "            self.ws.append(self.w[0])\n",
        "            self.history.append(total_loss)\n",
        "            \n",
        "        print('GD final loss (iteration {}): {:.4f}'.format(i+1, total_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "qohLamtmGm620GtzvTIhA",
      "metadata": {},
      "source": [
        "### Testing the training algorithm on a synthetic dataset\n",
        "The example uses univariate data linear (one input feature - the slope) model with gaussian noise. \n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "Em5HwS7hpPR5F7kWhSmGt",
      "metadata": {},
      "source": [
        "np.random.seed(0)\n",
        "numSamples = 1000\n",
        "slope = -0.9\n",
        "intercept = 0.4\n",
        "testsetSize= 400\n",
        "Xtrain = np.random.normal(size = (numSamples, 1))\n",
        "Ytrain = slope * Xtrain[:,0] + intercept * np.random.normal(size=Xtrain.shape[0])\n",
        "\n",
        "Xtest = np.sort(np.random.normal(size = (testsetSize, 1)))\n",
        "Ytest = slope * Xtest[:,0] + intercept * np.random.normal(size=Xtest.shape[0])\n",
        "\n",
        "plt.plot(Xtrain, Ytrain, 'g.');\n",
        "plt.plot(Xtest, Ytest, 'b.');\n",
        "\n",
        "plt.xlabel(\"Input feature\")\n",
        "plt.ylabel(\"Predicted output\")\n",
        "plt.title(\"Synthetic dataset\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "rrauB5EchziuIEdyO2Wzo",
      "metadata": {},
      "source": [
        "You can experiment with how the learning rate impacts the algorithm's convergence. A very low learning rate leads to slow convergence, as the algorithm takes small steps towards the optimal solution. Conversely, a very high learning rate can also hinder convergence, either by causing the algorithm to oscillate and \"jump around\" too much, or by preventing it from converging entirely.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "tpxyJyoz7fEhyaSQUJGHL",
      "metadata": {},
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "aaDR2exLrnaOwOau1dN8i",
      "metadata": {},
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "IYrwOo4nNrzgQR80fdJ_4",
      "metadata": {},
      "source": [
        "gd_regression = LeastSquaresRegressorGD(n_iter=20, learningRate=0.25, tolerance=0.005)\n",
        "gd_regression.fit(Xtrain, Ytrain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "G-yoJcw2JNrMPNQXHwlk1",
      "metadata": {},
      "source": [
        "The learning rate in the example is deliberately set low to clearly illustrate how each iteration improves the fit. Adjusting the learning rate to approximately 0.1 will result in much faster convergence.\n",
        "Plotting the `history`\n",
        "  during training reveals the progress of the optimization \n",
        "The gradient descent algorithm performs reasonably well (but poorly compared to more sophisticated models). That is unsurprising since the algortihm  has not made any updates by that point.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "qNbSFFEvxyXTTEkSyTiyy",
      "metadata": {},
      "source": [
        "plt.plot(gd_regression.history, '.-')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "jvkfpEQuqIFtMtDMK2pYN",
      "metadata": {},
      "source": [
        "The evaluation of the MSE on the test set reveals that the model found by the training algorithm is reasonably good.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "nzRk4s5cdLYq9YUYI3Qj9",
      "metadata": {},
      "source": [
        "print(mean_squared_error(Ytest, gd_regression.predict(Xtest)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "dUj54b4S43zRpw7RMb30w",
      "metadata": {},
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "MHk24wvbbUa6fm6T8cxa7",
      "metadata": {},
      "source": [
        "for i, theta in enumerate(gd_regression.ws):\n",
        "    # Show only the first 10 figures due\n",
        "    if(i>10):\n",
        "        break\n",
        "    gd_regression.theta = theta\n",
        "    y1 =  gd_regression.predict(np.array(-3))\n",
        "    y2 =  gd_regression.predict(np.array(3))\n",
        "    \n",
        "    plt.figure()\n",
        "\n",
        "    plt.plot(Xtrain, Ytrain, 'g.');\n",
        "    plt.plot(Xtest, Ytest, 'b.');\n",
        "\n",
        "    plt.xlabel(f\"Input feature ($w$)\")\n",
        "    plt.ylabel(\"Predicted output\")\n",
        "    plt.title(r\"Synthetic dataset (itr = %d, $w=$ %2.4f)\" % (i,gd_regression.w))\n",
        "    \n",
        "        \n",
        "    plt.plot([-3,3],[y1,y2],'r-')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "_kO1qQ28o4PLXTQR8aubS",
      "metadata": {},
      "source": [
        ""
      ]
    }
  ]
}